{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como no hay forma de que funcione la función recomendación, trabajo con una muestra calculada con la calculadoras de https://www.questionpro.com/es/calculadora-de-muestra.html\n",
    "\n",
    "![calculadora](../Recomendacion_peliculas/assets/calculo%20de%20muestra.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendacion(titulo: str):\n",
    "    # Ruta relativa al archivo\n",
    "    movies_path = os.path.join('Datasets', 'muestra_movies.csv')\n",
    "    combined_features_path = os.path.join('Datasets', 'muestra_features_optimized.npz')\n",
    "\n",
    "    # Cargar las bases de datos\n",
    "    movies = pd.read_csv(movies_path)\n",
    "    combined_features = np.load(combined_features_path)['arr_0']\n",
    "    \n",
    "    # Encuentra el índice de la película en el DataFrame\n",
    "    try:\n",
    "        idx = movies[movies['title'].str.lower() == titulo.lower()].index[0]\n",
    "    except IndexError:\n",
    "        return [{\"error\": f\"La película '{titulo}' no se encuentra en el dataset.\"}]\n",
    "\n",
    "    # Calcular la similitud de coseno entre la película seleccionada y todas las demás\n",
    "    similitudes = cosine_similarity(combined_features[idx].reshape(1, -1), combined_features)\n",
    "\n",
    "    # Ordenar las películas por similitud (exceptuando la película actual)\n",
    "    indices_similares = similitudes[0].argsort()[::-1][1:6]  # Toma las 5 más similares, excluyendo la misma\n",
    "\n",
    "    # Generar la lista de recomendaciones\n",
    "    recomendaciones = [\n",
    "        {\n",
    "            \"titulo\": movies.iloc[i]['title'],\n",
    "            \"similitud\": float(similitudes[0][i])\n",
    "        }\n",
    "        for i in indices_similares\n",
    "    ]\n",
    "\n",
    "    return recomendaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrecomendacion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAmong Friends\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mrecomendacion\u001b[1;34m(titulo)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa película \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitulo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m no se encuentra en el dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calcular la similitud de coseno entre la película seleccionada y todas las demás\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m similitudes \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Ordenar las películas por similitud (exceptuando la película actual)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m indices_similares \u001b[38;5;241m=\u001b[39m similitudes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margsort()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m6\u001b[39m]  \u001b[38;5;66;03m# Toma las 5 más similares, excluyendo la misma\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1679\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m \n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1679\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:194\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    186\u001b[0m         X,\n\u001b[0;32m    187\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 194\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "recomendacion('Among Friends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_2448\\2677896287.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movies_reduced['collection'] = movies_reduced['collection'].fillna('No Collection')\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_2448\\2677896287.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movies_reduced['genres_encoded'] = movies_reduced['genres_encoded'].apply(process_genres)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_2448\\2677896287.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movies_reduced['collection_weighted'] = movies_reduced['collection'].map(collection_counts).fillna(0)\n",
      "C:\\Users\\paula\\AppData\\Local\\Temp\\ipykernel_2448\\2677896287.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movies_reduced['collection_weighted_scaled'] = scaler.fit_transform(movies_reduced[['collection_weighted']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones finales de las características: (45376, 22)\n",
      "Archivos generados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "import ast\n",
    "\n",
    "# Leer el archivo original y seleccionar las columnas necesarias\n",
    "movies = pd.read_csv(r'Datasets/transformed_movies.csv')\n",
    "movies_reduced = movies[['title', 'popularity', 'collection', 'genres_encoded']]\n",
    "\n",
    "# Rellenar valores nulos en la columna de colecciones\n",
    "movies_reduced['collection'] = movies_reduced['collection'].fillna('No Collection')\n",
    "\n",
    "# Asegurarse de que los géneros estén en formato de lista\n",
    "def process_genres(genre_str):\n",
    "    try:\n",
    "        return ast.literal_eval(genre_str)\n",
    "    except ValueError:\n",
    "        return []\n",
    "\n",
    "movies_reduced['genres_encoded'] = movies_reduced['genres_encoded'].apply(process_genres)\n",
    "\n",
    "# Procesar géneros usando MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_matrix = mlb.fit_transform(movies_reduced['genres_encoded'])\n",
    "movies_features = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "\n",
    "# Procesar colecciones como una sola variable escalada\n",
    "collection_counts = movies_reduced['collection'].value_counts()\n",
    "movies_reduced['collection_weighted'] = movies_reduced['collection'].map(collection_counts).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "movies_reduced['collection_weighted_scaled'] = scaler.fit_transform(movies_reduced[['collection_weighted']])\n",
    "\n",
    "# Combinar características: géneros y colecciones escaladas\n",
    "movies_features['collection_scaled'] = movies_reduced['collection_weighted_scaled'].values\n",
    "\n",
    "# Agregar popularidad como una característica más\n",
    "movies_features['popularity'] = movies_reduced['popularity'].values\n",
    "\n",
    "# Verificar dimensiones finales\n",
    "print(\"Dimensiones finales de las características:\", movies_features.shape)\n",
    "\n",
    "# Guardar la matriz combinada en un archivo comprimido\n",
    "np.savez_compressed(\"Datasets/muestra_combined_features.npz\", movies_features.values)\n",
    "\n",
    "# Guardar el DataFrame reducido como CSV\n",
    "movies_reduced[['title', 'popularity', 'collection', 'genres_encoded']].to_csv(r'Datasets/movies_reduced.csv', index=False)\n",
    "\n",
    "print(\"Archivos generados exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones finales de las características: (45376, 22)\n",
      "Archivos generados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Crear una copia del DataFrame para evitar el SettingWithCopyWarning\n",
    "movies_reduced = movies[['title', 'popularity', 'collection', 'genres_encoded']].copy()\n",
    "\n",
    "# Rellenar valores nulos en la columna de colecciones\n",
    "movies_reduced['collection'] = movies_reduced['collection'].fillna('No Collection')\n",
    "\n",
    "# Asegurarse de que los géneros estén en formato de lista\n",
    "def process_genres(genre_str):\n",
    "    try:\n",
    "        return ast.literal_eval(genre_str)\n",
    "    except ValueError:\n",
    "        return []\n",
    "\n",
    "movies_reduced['genres_encoded'] = movies_reduced['genres_encoded'].apply(process_genres)\n",
    "\n",
    "# Procesar géneros usando MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_matrix = mlb.fit_transform(movies_reduced['genres_encoded'])\n",
    "movies_features = pd.DataFrame(genres_matrix, columns=mlb.classes_)\n",
    "\n",
    "# Procesar colecciones como una sola variable escalada\n",
    "collection_counts = movies_reduced['collection'].value_counts()\n",
    "movies_reduced['collection_weighted'] = movies_reduced['collection'].map(collection_counts).fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "movies_reduced['collection_weighted_scaled'] = scaler.fit_transform(movies_reduced[['collection_weighted']])\n",
    "\n",
    "# Combinar características: géneros y colecciones escaladas\n",
    "movies_features['collection_scaled'] = movies_reduced['collection_weighted_scaled'].values\n",
    "\n",
    "# Agregar popularidad como una característica más\n",
    "movies_features['popularity'] = movies_reduced['popularity'].values\n",
    "\n",
    "# Verificar dimensiones finales\n",
    "print(\"Dimensiones finales de las características:\", movies_features.shape)\n",
    "\n",
    "# Guardar la matriz combinada en un archivo comprimido\n",
    "np.savez_compressed(\"Datasets/muestra_combined_features.npz\", movies_features.values)\n",
    "\n",
    "# Guardar el DataFrame reducido como CSV\n",
    "movies_reduced[['title', 'popularity', 'collection', 'genres_encoded']].to_csv(r'Datasets/movies_reduced.csv', index=False)\n",
    "\n",
    "print(\"Archivos generados exitosamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de collection_encoded: (45000, 1691)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensiones de collection_encoded:\", collection_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de valores únicos en 'collection': 1691\n",
      "Primeros 10 valores únicos:\n",
      " ['No Collection' 'Friday the 13th Collection' 'American Pie Collection'\n",
      " ... 'Journey to the West Collection' 'Inuyasha Collection'\n",
      " 'Miles Jupp Standup Collection']\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de valores únicos en 'collection':\", muestra_movies['collection'].nunique())\n",
    "print(\"Primeros 10 valores únicos:\\n\", muestra_movies['collection'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones finales de las características escaladas: (12186, 905)\n",
      "Archivo comprimido guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Convertir todos los nombres de las columnas a cadenas\n",
    "movies_features.columns = movies_features.columns.astype(str)\n",
    "\n",
    "# Escalar todas las características\n",
    "scaler = StandardScaler()\n",
    "movies_features_scaled = scaler.fit_transform(movies_features)\n",
    "\n",
    "# Verificar dimensiones finales\n",
    "print(\"Dimensiones finales de las características escaladas:\", movies_features_scaled.shape)\n",
    "\n",
    "# Guardar la matriz combinada\n",
    "np.savez_compressed(\"Datasets/muestra_combined_features.npz\", movies_features_scaled)\n",
    "\n",
    "print(\"Archivo comprimido guardado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendacion(titulo: str):\n",
    "    # Ruta relativa al archivo\n",
    "    movies_path = os.path.join('Datasets', 'movies_reduced.csv')\n",
    "    combined_features_path = os.path.join('Datasets', 'muestra_combined_features.npz')\n",
    "\n",
    "    # Cargar las bases de datos\n",
    "    movies = pd.read_csv(movies_path)\n",
    "    combined_features = np.load(combined_features_path)['arr_0']\n",
    "    \n",
    "    # Encuentra el índice de la película en el DataFrame\n",
    "    try:\n",
    "        idx = movies[movies['title'].str.lower() == titulo.lower()].index[0]\n",
    "    except IndexError:\n",
    "        return [{\"error\": f\"La película '{titulo}' no se encuentra en el dataset.\"}]\n",
    "\n",
    "    # Calcular la similitud de coseno entre la película seleccionada y todas las demás\n",
    "    similitudes = cosine_similarity(combined_features[idx].reshape(1, -1), combined_features)\n",
    "\n",
    "    # Ordenar las películas por similitud (exceptuando la película actual)\n",
    "    indices_similares = similitudes[0].argsort()[::-1][1:6]  # Toma las 5 más similares, excluyendo la misma\n",
    "\n",
    "    # Generar la lista de recomendaciones\n",
    "    recomendaciones = [\n",
    "        {\n",
    "            \"titulo\": movies.iloc[i]['title'],\n",
    "            \"similitud\": float(similitudes[0][i])\n",
    "        }\n",
    "        for i in indices_similares\n",
    "    ]\n",
    "\n",
    "    return recomendaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'titulo': 'Meet the Parents', 'similitud': 0.9999942393515939},\n",
       " {'titulo': 'Meet the Fockers', 'similitud': 0.9989368653504107},\n",
       " {'titulo': 'American Pie', 'similitud': 0.9988592742766998},\n",
       " {'titulo': 'Grumpier Old Men', 'similitud': 0.998762763998953},\n",
       " {'titulo': 'The Prince & Me', 'similitud': 0.9986698135857679}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendacion('American Wedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar todas las características\n",
    "scaler = StandardScaler()\n",
    "movies_features_scaled = scaler.fit_transform(movies_features)\n",
    "\n",
    "# Multiplicar la columna `collection_encoded` para ajustar su peso\n",
    "movies_features_scaled[:, -1] *= 1000  # `collection_encoded` es la última columna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
